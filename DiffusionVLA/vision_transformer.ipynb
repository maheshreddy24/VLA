{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e3cb15ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as T\n",
    "from torch.optim import Adam\n",
    "from torchvision.datasets.mnist import MNIST\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from torch.optim import Adam\n",
    "\n",
    "from torchvision.datasets.mnist import MNIST\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "48cae8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "  def __init__(self, d_model, img_size, patch_size, n_channels):\n",
    "    super().__init__()\n",
    "\n",
    "    self.d_model = d_model # Dimensionality of Model\n",
    "    self.img_size = img_size # Image Size\n",
    "    self.patch_size = patch_size # Patch Size\n",
    "    self.n_channels = n_channels # Number of Channels\n",
    "\n",
    "    self.linear_project = nn.Conv2d(self.n_channels, self.d_model, kernel_size=self.patch_size, stride=self.patch_size)\n",
    "\n",
    "  # B: Batch Size\n",
    "  # C: Image Channels\n",
    "  # H: Image Height\n",
    "  # W: Image Width\n",
    "  # P_col: Patch Column\n",
    "  # P_row: Patch Row\n",
    "  def forward(self, x):\n",
    "    x = self.linear_project(x) # (B, C, H, W) -> (B, d_model, P_col, P_row)\n",
    "\n",
    "    x = x.flatten(2) # (B, d_model, P_col, P_row) -> (B, d_model, P)\n",
    "\n",
    "    x = x.transpose(1, 2) # (B, d_model, P) -> (B, P, d_model)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe84c302",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "  def __init__(self, d_model, max_seq_length):\n",
    "    super().__init__()\n",
    "\n",
    "    self.cls_token = nn.Parameter(torch.randn(1, 1, d_model)) # Classification Token\n",
    "\n",
    "    # Creating positional encoding\n",
    "    pe = torch.zeros(max_seq_length, d_model)\n",
    "\n",
    "    for pos in range(max_seq_length):\n",
    "      for i in range(d_model):\n",
    "        if i % 2 == 0:\n",
    "          pe[pos][i] = np.sin(pos/(10000 ** (i/d_model)))\n",
    "        else:\n",
    "          pe[pos][i] = np.cos(pos/(10000 ** ((i-1)/d_model)))\n",
    "\n",
    "    self.register_buffer('pe', pe.unsqueeze(0))\n",
    "\n",
    "  def forward(self, x):\n",
    "    # Expand to have class token for every image in batch\n",
    "    tokens_batch = self.cls_token.expand(x.size()[0], -1, -1)\n",
    "\n",
    "    # Adding class tokens to the beginning of each embedding\n",
    "    x = torch.cat((tokens_batch,x), dim=1)\n",
    "\n",
    "    # Add positional encoding to embeddings\n",
    "    x = x + self.pe\n",
    "\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "493386ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "  def __init__(self, d_model, head_size):\n",
    "    super().__init__()\n",
    "    self.head_size = head_size\n",
    "\n",
    "    self.query = nn.Linear(d_model, head_size)\n",
    "    self.key = nn.Linear(d_model, head_size)\n",
    "    self.value = nn.Linear(d_model, head_size)\n",
    "\n",
    "  def forward(self, x):\n",
    "    # Obtaining Queries, Keys, and Values\n",
    "    Q = self.query(x)\n",
    "    K = self.key(x)\n",
    "    V = self.value(x)\n",
    "\n",
    "    # Dot Product of Queries and Keys\n",
    "    attention = Q @ K.transpose(-2,-1)\n",
    "\n",
    "    # Scaling\n",
    "    attention = attention / (self.head_size ** 0.5)\n",
    "\n",
    "    attention = torch.softmax(attention, dim=-1)\n",
    "\n",
    "    attention = attention @ V\n",
    "\n",
    "    return attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4824ac65",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "  def __init__(self, d_model, n_heads):\n",
    "    super().__init__()\n",
    "    self.head_size = d_model // n_heads\n",
    "\n",
    "    self.W_o = nn.Linear(d_model, d_model)\n",
    "\n",
    "    self.heads = nn.ModuleList([AttentionHead(d_model, self.head_size) for _ in range(n_heads)])\n",
    "\n",
    "  def forward(self, x):\n",
    "    # Combine attention heads\n",
    "    out = torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "\n",
    "    out = self.W_o(out)\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3c72cf65",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "  def __init__(self, d_model, n_heads, r_mlp=4):\n",
    "    super().__init__()\n",
    "    self.d_model = d_model\n",
    "    self.n_heads = n_heads\n",
    "\n",
    "    # Sub-Layer 1 Normalization\n",
    "    self.ln1 = nn.LayerNorm(d_model)\n",
    "\n",
    "    # Multi-Head Attention\n",
    "    self.mha = MultiHeadAttention(d_model, n_heads)\n",
    "\n",
    "    # Sub-Layer 2 Normalization\n",
    "    self.ln2 = nn.LayerNorm(d_model)\n",
    "\n",
    "    # Multilayer Perception\n",
    "    self.mlp = nn.Sequential(\n",
    "        nn.Linear(d_model, d_model*r_mlp),\n",
    "        nn.GELU(),\n",
    "        nn.Linear(d_model*r_mlp, d_model)\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    # Residual Connection After Sub-Layer 1\n",
    "    out = x + self.mha(self.ln1(x))\n",
    "\n",
    "    # Residual Connection After Sub-Layer 2\n",
    "    out = out + self.mlp(self.ln2(out))\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "597bb8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "  def __init__(self, d_model, n_classes, img_size, patch_size, n_channels, n_heads, n_layers):\n",
    "    super().__init__()\n",
    "\n",
    "    assert img_size[0] % patch_size[0] == 0 and img_size[1] % patch_size[1] == 0, \"img_size dimensions must be divisible by patch_size dimensions\"\n",
    "    assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n",
    "\n",
    "    self.d_model = d_model # Dimensionality of model\n",
    "    self.n_classes = n_classes # Number of classes\n",
    "    self.img_size = img_size # Image size\n",
    "    self.patch_size = patch_size # Patch size\n",
    "    self.n_channels = n_channels # Number of channels\n",
    "    self.n_heads = n_heads # Number of attention heads\n",
    "\n",
    "    self.n_patches = (self.img_size[0] * self.img_size[1]) // (self.patch_size[0] * self.patch_size[1])\n",
    "    self.max_seq_length = self.n_patches + 1\n",
    "\n",
    "    self.patch_embedding = PatchEmbedding(self.d_model, self.img_size, self.patch_size, self.n_channels)\n",
    "    self.positional_encoding = PositionalEncoding( self.d_model, self.max_seq_length)\n",
    "    self.transformer_encoder = nn.Sequential(*[TransformerEncoder( self.d_model, self.n_heads) for _ in range(n_layers)])\n",
    "\n",
    "    # Classification MLP\n",
    "    self.classifier = nn.Sequential(\n",
    "        nn.Linear(self.d_model, self.n_classes),\n",
    "        nn.Softmax(dim=-1)\n",
    "    )\n",
    "\n",
    "  def forward(self, images):\n",
    "    x = self.patch_embedding(images)\n",
    "\n",
    "    x = self.positional_encoding(x)\n",
    "\n",
    "    x = self.transformer_encoder(x)\n",
    "    \n",
    "    x = self.classifier(x[:,0])\n",
    "\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ff933543",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9.91M/9.91M [00:03<00:00, 2.83MB/s]\n",
      "100%|██████████| 28.9k/28.9k [00:00<00:00, 127kB/s]\n",
      "100%|██████████| 1.65M/1.65M [00:01<00:00, 1.24MB/s]\n",
      "100%|██████████| 4.54k/4.54k [00:00<00:00, 4.71MB/s]\n"
     ]
    }
   ],
   "source": [
    "d_model = 9\n",
    "n_classes = 10\n",
    "img_size = (32,32)\n",
    "patch_size = (16,16)\n",
    "n_channels = 1\n",
    "n_heads = 3\n",
    "n_layers = 3\n",
    "batch_size = 128\n",
    "epochs = 5\n",
    "alpha = 0.005\n",
    "transform = T.Compose([\n",
    "  T.Resize(img_size),\n",
    "  T.ToTensor()\n",
    "])\n",
    "\n",
    "train_set = MNIST(\n",
    "  root=\"./../datasets\", train=True, download=True, transform=transform\n",
    ")\n",
    "test_set = MNIST(\n",
    "  root=\"./../datasets\", train=False, download=True, transform=transform\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_set, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_set, shuffle=False, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2ba03dd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mahesh/miniconda3/envs/lerobot/lib/python3.10/site-packages/torch/cuda/__init__.py:174: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at /pytorch/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device:  cpu \n",
      "Epoch 1/5 loss: 1.743\n",
      "Epoch 2/5 loss: 1.627\n",
      "Epoch 3/5 loss: 1.570\n",
      "Epoch 4/5 loss: 1.560\n",
      "Epoch 5/5 loss: 1.551\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device: \", device, f\"({torch.cuda.get_device_name(device)})\" if torch.cuda.is_available() else \"\")\n",
    "\n",
    "transformer = VisionTransformer(d_model, n_classes, img_size, patch_size, n_channels, n_heads, n_layers).to(device)\n",
    "\n",
    "optimizer = Adam(transformer.parameters(), lr=alpha)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "  training_loss = 0.0\n",
    "  for i, data in enumerate(train_loader, 0):\n",
    "    inputs, labels = data\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    outputs = transformer(inputs)\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    training_loss += loss.item()\n",
    "\n",
    "  print(f'Epoch {epoch + 1}/{epochs} loss: {training_loss  / len(train_loader) :.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e2ceb9fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Accuracy: 91 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "  for data in test_loader:\n",
    "    images, labels = data\n",
    "    images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "    outputs = transformer(images)\n",
    "\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum().item()\n",
    "  print(f'\\nModel Accuracy: {100 * correct // total} %')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "116022d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.1+cu126\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mahesh/miniconda3/envs/lerobot/lib/python3.10/site-packages/torch/cuda/__init__.py:174: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at /pytorch/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available())\n\u001b[1;32m      4\u001b[0m a \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m4\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m32\u001b[39m,\u001b[38;5;241m32\u001b[39m) \n\u001b[0;32m----> 5\u001b[0m a \u001b[38;5;241m=\u001b[39m \u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda:0\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/lerobot/lib/python3.10/site-packages/torch/cuda/__init__.py:372\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron:\n\u001b[1;32m    371\u001b[0m     os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLAZY\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 372\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;66;03m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;66;03m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;66;03m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[1;32m    376\u001b[0m _tls\u001b[38;5;241m.\u001b[39mis_initializing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n",
    "a = torch.randn(4,3,32,32) \n",
    "a = a.to('cuda:0')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lerobot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
